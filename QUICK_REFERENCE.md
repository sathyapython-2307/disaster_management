# Quick Reference - Data Import System

## üöÄ Quick Start (5 Minutes)

### 1. Prepare Your File
Create `disasters.csv`:
```csv
disaster_type,location_name,latitude,longitude,risk_score
Earthquake,San Francisco,37.77,-122.42,85.5
Flood,New York,40.71,-74.01,72.3
Wildfire,Los Angeles,34.05,-118.24,68.9
```

### 2. Upload File
```bash
curl -X POST http://localhost:8000/api/data-sources/upload/ \
  -F "file=@disasters.csv" \
  -F "source_type=csv" \
  -H "X-CSRFToken: $CSRF_TOKEN"
```

Response:
```json
{
  "file_path": "uploads/data_sources/abc123.csv",
  "original_name": "disasters.csv",
  "size": 156
}
```

### 3. Create Data Source
```bash
curl -X POST http://localhost:8000/api/data-sources/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "My Disasters",
    "source_type": "csv",
    "file_path": "uploads/data_sources/abc123.csv",
    "sync_interval_minutes": 60
  }'
```

### 4. Sync Data
```bash
# Command line
python manage.py sync_data_sources --all

# Or via API
curl -X POST http://localhost:8000/api/data-sources/<id>/sync/
```

### 5. View Results
Go to http://localhost:8000/disasters/ to see imported events

---

## üìÅ File Formats Supported

### CSV
```csv
disaster_type,location_name,latitude,longitude,risk_score
Earthquake,San Francisco,37.77,-122.42,85.5
```

### JSON
```json
[
  {
    "disaster_type": "Earthquake",
    "location_name": "San Francisco",
    "latitude": 37.77,
    "longitude": -122.42,
    "risk_score": 85.5
  }
]
```

### XML
```xml
<disasters>
  <event>
    <disaster_type>Earthquake</disaster_type>
    <location_name>San Francisco</location_name>
    <latitude>37.77</latitude>
    <longitude>-122.42</longitude>
    <risk_score>85.5</risk_score>
  </event>
</disasters>
```

### TXT
```
Line 1: Earthquake - San Francisco
Line 2: Flood - New York
Line 3: Wildfire - Los Angeles
```

---

## üîß Management Command

```bash
# List available sources
python manage.py sync_data_sources

# Sync all active sources
python manage.py sync_data_sources --all

# Sync specific source by ID
python manage.py sync_data_sources --source-id <UUID>

# Sync specific source by name
python manage.py sync_data_sources --source-name "Source Name"

# Force sync (ignore interval)
python manage.py sync_data_sources --all --force

# Sync as specific user (for audit)
python manage.py sync_data_sources --all --user admin
```

---

## üåê API Endpoints

### Upload File
```
POST /api/data-sources/upload/
Content-Type: multipart/form-data

Fields:
  - file: <file>
  - source_type: csv|json|xml|txt

Response: {file_path, original_name, size}
```

### Create Data Source
```
POST /api/data-sources/
Content-Type: application/json

Body: {
  name: "Source Name",
  source_type: "csv|json|xml|file|database|api",
  file_path: "uploads/data_sources/...",
  sync_interval_minutes: 60
}

Response: {id, name, ...}
```

### Sync Single Source
```
POST /api/data-sources/{id}/sync/

Response: {status, processed, message, errors?}
```

### Sync All Sources
```
POST /api/data-sources/sync_all/

Response: {status, results: {total, synced, failed, details}}
```

### List Data Sources
```
GET /api/data-sources/

Response: [{id, name, source_type, last_sync, ...}, ...]
```

---

## üìä Field Mapping

The system automatically detects these column names:

| Target Field | Detected Names |
|---|---|
| disaster_type | disaster_type, type, event_type, disaster |
| location_name | location, location_name, place, area |
| latitude | latitude, lat, y |
| longitude | longitude, lon, long, x |
| risk_score | risk_score, risk, severity |
| confidence_level | confidence_level, confidence |
| magnitude | magnitude, mag |
| wind_speed_kmh | wind_speed_kmh, wind_speed, windspeed |
| rainfall_mm | rainfall_mm, rainfall, rain |
| predicted_time | predicted_time, time, timestamp, datetime |
| start_time | start_time, start |
| end_time | end_time, end |
| estimated_affected_population | estimated_affected_population, population, people |
| estimated_damage_usd | estimated_damage_usd, damage, cost |

**Required**: disaster_type, location_name

---

## ‚ö†Ô∏è Common Issues

### File not found
```
Error: FileNotFoundError: File not found
Solution: Check file_path in DataSource matches actual file location
```

### Type conversion error
```
Error: Could not convert field to type
Solution: Ensure numeric fields contain numbers (not "High", "Medium")
```

### Missing required fields
```
Error: Row X: Missing disaster_type or location_name
Solution: Ensure CSV has disaster_type and location_name columns
```

### Permission denied
```
Error: PermissionError
Solution: Check media/ directory permissions
```

---

## üìà Performance

| Operation | Time | Rate |
|---|---|---|
| CSV reading | 1 sec | 1000 rows/sec |
| JSON parsing | 2 sec | 500 objects/sec |
| DB import | 5 sec | 100 records/sec |
| Full sync (5 records) | 0.3 sec | - |

---

## üîê Security

- ‚úÖ Only CSV, JSON, XML, TXT allowed
- ‚úÖ Max file size: 2.5MB
- ‚úÖ Requires authentication
- ‚úÖ All actions logged with user
- ‚úÖ Django ORM prevents SQL injection

---

## üìù File Structure

```
core/
‚îú‚îÄ‚îÄ file_reader.py
‚îÇ   ‚îú‚îÄ‚îÄ FileReader (base)
‚îÇ   ‚îú‚îÄ‚îÄ CSVReader
‚îÇ   ‚îú‚îÄ‚îÄ JSONReader
‚îÇ   ‚îú‚îÄ‚îÄ XMLReader
‚îÇ   ‚îú‚îÄ‚îÄ TXTReader
‚îÇ   ‚îî‚îÄ‚îÄ FileReaderFactory
‚îú‚îÄ‚îÄ data_sync.py
‚îÇ   ‚îî‚îÄ‚îÄ DataSyncManager
‚îÇ       ‚îú‚îÄ‚îÄ sync_data_source()
‚îÇ       ‚îú‚îÄ‚îÄ _process_disaster_records()
‚îÇ       ‚îú‚îÄ‚îÄ _extract_disaster_data()
‚îÇ       ‚îî‚îÄ‚îÄ sync_all_active_sources()
‚îî‚îÄ‚îÄ management/commands/
    ‚îî‚îÄ‚îÄ sync_data_sources.py
```

---

## üß™ Testing

Run complete test:
```bash
python test_complete_workflow.py
```

Test output:
```
[1] Creating test user... ‚úì
[2] Reading CSV file... ‚úì
[3] File reader test... ‚úì (5 records)
[4] Simulating upload... ‚úì
[5] Creating DataSource... ‚úì
[6] Syncing data... ‚úì (5 records)
[7] Verifying disasters... ‚úì (5 created)
[8] Testing API endpoints... ‚úì

ALL TESTS PASSED ‚úì‚úì‚úì
```

---

## üö® Troubleshooting

### Check Django Logs
```bash
tail -f logs/disaster_dashboard.log | grep sync
```

### Verify File Exists
```bash
ls -la media/uploads/data_sources/
```

### Check Database
```bash
python manage.py shell
>>> from disasters.models import DisasterEvent
>>> DisasterEvent.objects.count()
```

### Test File Reading
```bash
python manage.py shell
>>> from core.file_reader import FileReaderFactory
>>> data = FileReaderFactory.read_file('path/to/file.csv')
>>> len(data)
```

### Check Audit Logs
```bash
python manage.py shell
>>> from core.models import AuditLog
>>> AuditLog.objects.filter(resource_type='DataSync').count()
```

---

## üìö Full Documentation

For complete documentation, see:
- `DATA_IMPORT_SYNC_GUIDE.md` - Comprehensive guide
- `PROJECT_ANALYSIS_IMPLEMENTATION.md` - Implementation summary
- Inline docstrings in code

---

## üéØ Next Steps

1. ‚úÖ Understand the data format your files use
2. ‚úÖ Prepare test file (use CSV for easiest start)
3. ‚úÖ Upload file via API or web UI
4. ‚úÖ Create DataSource record
5. ‚úÖ Sync data
6. ‚úÖ Verify disasters appear in dashboard

---

**Last Updated**: December 15, 2025  
**Version**: 1.0  
**Status**: Production Ready ‚úÖ
